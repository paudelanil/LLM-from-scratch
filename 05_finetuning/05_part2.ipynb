{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.15)    # 15% for testing\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Test set length: 165\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\", \"w\") as json_file:\n",
    "    json.dump(train_data, json_file, indent=4)\n",
    "    \n",
    "with open(\"test.json\", \"w\") as json_file:\n",
    "    json.dump(test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]\n",
      "generation_config.json: 100%|███████████████████| 124/124 [00:00<00:00, 525kB/s]\u001b[A\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 7.34k/7.34k [00:00<00:00, 18.3MB/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/2.11M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors.index.json:   0%|                 | 0.00/35.7k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 735/735 [00:00<00:00, 4.15MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Fetching 7 files:  14%|███▊                       | 1/7 [00:01<00:09,  1.59s/it]\n",
      "\n",
      "model.safetensors.index.json: 100%|█████████| 35.7k/35.7k [00:00<00:00, 177kB/s]\u001b[A\u001b[A\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:01<00:00, 1.84MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|              | 0.00/564M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   2%|     | 10.5M/564M [00:03<02:56, 3.14MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/5.00G [00:05<44:51, 1.85MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 21.0M/564M [00:06<02:51, 3.17MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/5.00G [00:10<41:17, 2.01MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 31.5M/564M [00:10<03:10, 2.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 41.9M/564M [00:15<03:16, 2.65MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 31.5M/5.00G [00:15<41:19, 2.00MB/s]\u001b[A^C\n",
      "Fetching 7 files:  29%|███████▋                   | 2/7 [00:23<00:58, 11.61s/it]\n"
     ]
    }
   ],
   "source": [
    "!litgpt finetune_lora microsoft/phi-2 \\\n",
    "--data JSON \\\n",
    "--data.val_split_fraction 0.1 \\\n",
    "--data.json_path train.json \\\n",
    "--train.epochs 3 \\\n",
    "--train.log_interval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n"
     ]
    }
   ],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "print(format_input(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "checkpoint_dir '/home/anil/Documents/AI_Fellowship/LLMs/05_finetuning/checkpoints/microsoft/phi-2' is missing the files: ['lit_model.pth', 'model_config.yaml'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitgpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/LLMs/lib/python3.10/site-packages/litgpt/api.py:180\u001b[0m, in \u001b[0;36mLLM.load\u001b[0;34m(cls, model, init, tokenizer_dir, access_token, distribute)\u001b[0m\n\u001b[1;32m    177\u001b[0m allowed_init \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 180\u001b[0m     checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[43mauto_download_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_tokenizer_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mfrom_file(checkpoint_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/LLMs/lib/python3.10/site-packages/litgpt/utils.py:602\u001b[0m, in \u001b[0;36mauto_download_checkpoint\u001b[0;34m(model_name, access_token, ignore_tokenizer_files)\u001b[0m\n\u001b[1;32m    600\u001b[0m         checkpoint_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m checkpoint_dir\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m checkpoint_dir\n",
      "File \u001b[0;32m~/miniforge3/envs/LLMs/lib/python3.10/site-packages/litgpt/utils.py:593\u001b[0m, in \u001b[0;36mauto_download_checkpoint\u001b[0;34m(model_name, access_token, ignore_tokenizer_files)\u001b[0m\n\u001b[1;32m    591\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m extend_checkpoint_dir(Path(model_name))\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[43mcheck_valid_checkpoint_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_tokenizer_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_tokenizer_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m access_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/LLMs/lib/python3.10/site-packages/litgpt/utils.py:127\u001b[0m, in \u001b[0;36mcheck_valid_checkpoint_dir\u001b[0;34m(checkpoint_dir, model_filename, verbose, raise_error, ignore_tokenizer_files)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(error_message, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_dir \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(checkpoint_dir\u001b[38;5;241m.\u001b[39mabsolute())\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mproblem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: checkpoint_dir '/home/anil/Documents/AI_Fellowship/LLMs/05_finetuning/checkpoints/microsoft/phi-2' is missing the files: ['lit_model.pth', 'model_config.yaml']."
     ]
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
